TF-IDF Term Fre	uency - Inverse Document Frequency: Words that appear in may documents are probably less meaningful
tfidf = word count / document count.
In practise we may use some log and smoothening

Word Analogy:
1) King - Queen = Prince - Princess
2) Japan - Japanese = China - Chinese


How to find the word analogy?

closest_distance = infinity
best_word = None
test_vector = king - man + woman

for word, vector in vocabulary:
	distance = get_distance(test_vector, vector)
	if (distance < closest_distance):
		best_word = word
		closest_distance = distance

We generally use cosine distance
cosine distance = 1- (a.b/|a||b|)

t-SNE converts word to vector from V X D to V X 2

# Bag of Words model : We represent the sentence as a vector
We dont consider the order of word in a sentence
eg: (toy dog) same as (dog toy)

sentence = I am good
feture vector = vec("I") + vec("am") + vec("good") / 3


Language Modelling and Neural Netowork

Language Model is the model of probabilities of sequence of words

Bigram Model = p(Wt | Wt-1)
eg: P("brown"|"quick") = 0.5 but P("the"|"the") = 0.

P("brown"|"quick") = count("brown"->"quick")|count("quick")

P(ABCDE) = P(E|D).P(D|C).P(C|B).P(B|A).P(A)

Neural Bigram Model

* One hot encoding
a = [1 0 0]
b = [0 1 0]
c = [0 0 1]

p(y|x) = softmax(Wx)
where x is current word and y the next word.


cbow continuous bag of words

# Hierarchical softmax
Large number of output possibility makes chance of error higher. The ourput is from the vocabulary. Assume we have 300000 words in the vocabuary, the function should choose one from this and so it is very high chance to have error.

Evaluating softmax is highly expensive O(Vocabulary).
Hierarchical softmax creates a binary tree and reduces its complexity to O(logn)
We use huffman coding to create the tree, which puts frequent words to the top and infrequent words to the bottom.

# Negative Sampling


F1 Score:
Harmonic Mean of precision and recall

RNN, HMM, Expectation Maximization
Maximum Likelyhood estimate
HMM = pi, A, B
pi = initial state (frequency of starting tag)
A = transition p(tag(t)|tag(t-1))
B = observation p(word(t)|tag(t))

POS tagging:
	1) Logistic Regression
	2) Recurrent Neural Network
	3) HMM

Logistic Regression:
	P(tag|word) = softmax(W[word_index])
	Demerit:
		logistic regression has a one to one mapping of words and tags. But there can be more than one tag for the same word.
		eg: I just love milk
			I want to milk the vistim of her money

Recurrent Neural Network
	It has a loop back in the hidden layer
	GRU and LSTM are modetn RNN
	It helps to retain long-term dependencies and avoid vanishing and exploding gradients

HMM
It is an unsupervised model
The probability of going from state 2 to state 3 only depends on state 2

Viterby Algorithm:
Context free Grammer
Left most derivative, Right most derivative


ID3
-P/(P+N) log(P|P+N) - N|(P+N) log(N|P+N)
Maximum Gain